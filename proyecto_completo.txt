=== ESTRUCTURA DEL PROYECTO ===
ProyectoAgents/
    .gitignore
    README.md
    run.py
    agent/
        llm_client.py
        mcp_tools.py
        orchestrator.py
        pdf_processor.py
    config/
        nebius_config.py
    ui/
        app.py
        components.py


=== CONTENIDO DE LOS ARCHIVOS ===


==================================================
ARCHIVO: README.md
==================================================

# üõ°Ô∏è Contract Guardian ‚Äî Auditor de Contratos con IA

**Contract Guardian** es una herramienta que analiza contratos de forma inteligente y te muestra, de manera visual, las cl√°usulas potencialmente abusivas o problem√°ticas antes de que firmes.

No es un abogado.  
No da asesoramiento legal profesional.  
Pero s√≠ te ayuda a **entender mejor lo que est√°s a punto de firmar**.

---

## üß† ¬øC√≥mo funciona?

1. **Subes o pegas tu contrato**  
   Puede ser un contrato de alquiler, laboral, de servicios o t√©rminos de uso.

2. **La IA divide el documento en cl√°usulas**  
   Cada p√°rrafo se analiza por separado.

3. **Clasificaci√≥n autom√°tica**  
   Un m√≥dulo especializado identifica el tipo de cada cl√°usula  
   (duraci√≥n, fianza, penalizaci√≥n, gastos, etc.).

4. **Consulta de leyes reales**  
   La IA usa herramientas externas para consultar art√≠culos legales relevantes  
   (por ejemplo: Ley de Arrendamientos Urbanos para contratos de alquiler).

5. **Comparaci√≥n autom√°tica**  
   La IA compara tu contrato con lo que establece la ley correspondiente.

6. **Informe visual**  
   Las cl√°usulas se muestran con colores:
   - üî¥ **Riesgo alto**: contradice un art√≠culo legal.
   - üü° **Riesgo medio**: lenguaje ambiguo o potencialmente abusivo.
   - üü¢ **Correcto**: no hay se√±ales de alerta.

---

## üéØ ¬øQu√© problemas detecta?

- Fianzas excesivas  
- Penalizaciones abusivas  
- Plazos ilegales  
- Cl√°usulas que renuncian a derechos  
- Obligaciones desproporcionadas  
- Ambig√ºedades que pueden perjudicarte  

---

## üß© ¬øQu√© tecnolog√≠as usa?

- **Un agente de IA** que planifica y razona paso a paso  
- **Herramientas MCP** que permiten consultar leyes y clasificar cl√°usulas  
- **Gradio** para la interfaz de usuario  
- **Bases legales** en formato estructurado que la IA usa para verificar informaci√≥n

---

## ‚ö†Ô∏è Importante

Contract Guardian es una herramienta informativa y experimental creada para un hackat√≥n.  
**No sustituye asesor√≠a profesional legal.**



==================================================
ARCHIVO: run.py
==================================================

#!/usr/bin/env python3
"""
run.py
Punto de entrada principal para Contract Guardian.
Maneja los imports correctamente.
"""
import os
import sys
from pathlib import Path

# A√±adir el directorio ra√≠z al path de Python
ROOT_DIR = Path(__file__).parent.resolve()
sys.path.insert(0, str(ROOT_DIR))

# Importar y lanzar la interfaz
try:
    from ui.app import demo
    print("üöÄ Iniciando Contract Guardian Agent...")
    print("üëâ Abre tu navegador en: http://127.0.0.1:7860")
    demo.queue().launch(server_name="0.0.0.0", server_port=7860, share=False)
except ImportError as e:
    print(f"‚ùå Error de importaci√≥n: {e}")
    print("Aseg√∫rate de estar ejecutando desde la ra√≠z del proyecto: python run.py")
except Exception as e:
    print(f"‚ùå Error inesperado: {e}")



==================================================
ARCHIVO: agent\llm_client.py
==================================================

#!/usr/bin/env python3
"""
agent/llm_client.py

Cliente de Nebius LLM para Contract Guardian Agent
Usa OpenAI-compatible client con Qwen3-30B-A3B-Thinking-2507
"""

import logging
from typing import Iterator, Optional
from openai import OpenAI
from config.nebius_config import (
    NEBIUS_API_BASE_URL,
    NEBIUS_API_KEY,
    NEBIUS_CONFIG,
    validate_config,
)

# ============================================================
# CONFIGURACI√ìN LOGGING
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================
# CLIENTE NEBIUS LLM
# ============================================================

class NebiumLLMClient:
    """Cliente para Nebius API con Qwen3."""
    
    def __init__(self):
        """Inicializa cliente de Nebius."""
        try:
            validate_config()
        except ValueError as e:
            logger.error(f"Config validation failed: {e}")
            raise
        
        self.client = OpenAI(
            base_url=NEBIUS_API_BASE_URL,
            api_key=NEBIUS_API_KEY,
        )
        
        logger.info(f"‚úÖ Nebius LLM Client initialized")
        logger.info(f"   Model: {NEBIUS_CONFIG['model']}")
        logger.info(f"   Base URL: {NEBIUS_API_BASE_URL}")
    

        # ... (m√©todos anteriores: analyze_contract, reason_about_clauses, etc.) ...

    def extract_search_terms(self, initial_analysis: str) -> str:
        """
        Pide al LLM que identifique los conceptos legales CLAVE para buscar en la base de datos.
        NO streaming, necesitamos la respuesta completa para procesarla.
        """
        system_prompt = """Eres un asistente legal experto en recuperaci√≥n de informaci√≥n.
Tu tarea es identificar conceptos legales clave para buscar en una base de datos de leyes espa√±olas (Estatuto de los Trabajadores, LAU, etc.).

SALIDA OBLIGATORIA: Solo una lista de 3 a 5 t√©rminos separados por comas. Sin explicaciones, sin puntos finales.
Ejemplo: "despido improcedente, fianza, duraci√≥n del contrato, preaviso"."""

        user_prompt = f"""Basado en este an√°lisis preliminar de un contrato, identifica los 3-5 t√©rminos legales m√°s cr√≠ticos que debemos verificar en la ley para confirmar si hay ilegalidades.

AN√ÅLISIS PRELIMINAR:
{initial_analysis[:2000]}  # Pasamos los primeros 2000 chars para contexto

T√âRMINOS DE B√öSQUEDA:"""

        try:
            logger.info("üîç Asking LLM for search terms...")
            response = self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.3,  # Baja temperatura para ser preciso
                max_tokens=50,
                stream=False      # No streaming, queremos el texto ya
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error extracting search terms: {e}")
            # Fallback por si falla el LLM
            return "terminaci√≥n, responsabilidad, pago"



    def analyze_contract(self, contract_text: str) -> Iterator[str]:
        """
        Analiza un contrato con streaming.
        
        Args:
            contract_text: Texto del contrato a analizar
            
        Yields:
            Chunks de an√°lisis del LLM (streaming)
        """
        
        if not contract_text or len(contract_text.strip()) < 50:
            logger.warning("Contract text too short")
            return
        
        system_prompt = """Eres un abogado experto en derecho espa√±ol con 20 a√±os de experiencia.

Tu tarea: Analizar contratos y identificar cl√°usulas riesgosas o ilegales.

AN√ÅLISIS A REALIZAR:
1. Identificar tipos de cl√°usulas (terminaci√≥n, pago, privacidad, etc.)
2. Detectar nivel de riesgo (ALTO/MEDIO/BAJO)
3. Se√±alar potenciales violaciones legales
4. Sugerir art√≠culos legales aplicables
5. Proporcionar recomendaciones

FORMATO RESPUESTA:
- S√© conciso pero preciso (m√°x 500 palabras)
- Estructura: Tipo | Riesgo | Problema | Art√≠culos | Recomendaci√≥n
- Usa markdown para claridad
- N√∫meros de cl√°usulas si las hay

TONO: Profesional, directo, sin alarmismo pero honesto sobre riesgos"""
        
        user_prompt = f"""Por favor, analiza este contrato e identifica cl√°usulas riesgosas o ilegales seg√∫n la ley espa√±ola.

CONTRATO:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{contract_text}

Proporciona un an√°lisis detallado de los riesgos legales encontrados."""
        
        try:
            logger.info("ü§ñ Sending analysis request to Nebius LLM (streaming)...")
            
            with self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=NEBIUS_CONFIG["temperature"],
                top_p=NEBIUS_CONFIG.get("top_p", 0.95),
                max_tokens=NEBIUS_CONFIG["max_tokens"],
                stream=True,
            ) as stream:
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        
        except Exception as e:
            logger.error(f"Error in analyze_contract: {e}")
            raise
    
    def reason_about_clauses(self, 
                            clauses_summary: str,
                            mcp_results: str) -> Iterator[str]:
        """
        Razona sobre cl√°usulas basado en resultados de MCP tools.
        
        Args:
            clauses_summary: Resumen de cl√°usulas detectadas
            mcp_results: Resultados de law_lookup + classify_clauses
            
        Yields:
            Chunks de razonamiento legal (streaming)
        """
        
        system_prompt = """Eres un abogado especialista en analizar y razonar sobre la legalidad de cl√°usulas contractuales.

Tu tarea: Dado un an√°lisis inicial y verificaci√≥n legal, genera razonamiento profundo sobre violaciones.

ESTRUCTURA RESPUESTA:
- Por cada cl√°usula problem√°tica:
  * Qu√© dice la cl√°usula
  * Qu√© dice la ley
  * Por qu√© es violaci√≥n o ilegal
  * Impacto legal
  * Recomendaci√≥n espec√≠fica"""
        
        user_prompt = f"""Bas√°ndote en estos resultados de an√°lisis legal, razona sobre por qu√© estas cl√°usulas son problem√°ticas:

RESUMEN CL√ÅUSULAS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{clauses_summary}

VERIFICACI√ìN LEGAL (de MCP tools):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{mcp_results}

Genera razonamiento detallado sobre la legalidad de cada cl√°usula."""
        
        try:
            logger.info("üß† Requesting legal reasoning from Nebius LLM (streaming)...")
            
            with self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=NEBIUS_CONFIG["temperature"],
                top_p=NEBIUS_CONFIG.get("top_p", 0.95),
                max_tokens=NEBIUS_CONFIG["max_tokens"],
                stream=True,
            ) as stream:
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        
        except Exception as e:
            logger.error(f"Error in reason_about_clauses: {e}")
            raise
    
    def generate_recommendations(self, analysis_data: str) -> Iterator[str]:
        """
        Genera recomendaciones personalizadas basadas en an√°lisis.
        
        Args:
            analysis_data: Datos de an√°lisis completo
            
        Yields:
            Chunks de recomendaciones (streaming)
        """
        
        system_prompt = """Eres un asesor legal experto en negociaci√≥n de contratos.

Tu tarea: Bas√°ndote en an√°lisis legal, generar recomendaciones pr√°cticas y accionables."""
        
        user_prompt = f"""Bas√°ndote en este an√°lisis, genera recomendaciones pr√°cticas para el cliente:

AN√ÅLISIS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{analysis_data}

Por favor proporciona:
1. Cl√°usulas a RECHAZAR (cr√≠ticas)
2. Cl√°usulas a NEGOCIAR (importantes)
3. Cl√°usulas ACEPTABLES (sin problemas)
4. Estrategia de negociaci√≥n recomendada"""
        
        try:
            logger.info("üí° Requesting recommendations from Nebius LLM (streaming)...")
            
            with self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=NEBIUS_CONFIG["temperature"],
                top_p=NEBIUS_CONFIG.get("top_p", 0.95),
                max_tokens=NEBIUS_CONFIG["max_tokens"],
                stream=True,
            ) as stream:
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        
        except Exception as e:
            logger.error(f"Error in generate_recommendations: {e}")
            raise


# ============================================================
# INSTANCIA GLOBAL
# ============================================================

llm_client: Optional[NebiumLLMClient] = None

def get_llm_client() -> NebiumLLMClient:
    """Obtiene o crea instancia del cliente LLM."""
    global llm_client
    if llm_client is None:
        llm_client = NebiumLLMClient()
    return llm_client


if __name__ == "__main__":
    # Test b√°sico
    client = get_llm_client()
    print("‚úÖ LLM Client initialized successfully")


==================================================
ARCHIVO: agent\mcp_tools.py
==================================================

#!/usr/bin/env python3
"""
agent/mcp_tools.py

Gestor de herramientas MCP.
Se encarga de hacer las llamadas HTTP a los servidores MCP (puertos 8001 y 8002).
"""

import requests
import logging
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

# ============================================================
# CONFIGURACI√ìN MCP SERVERS
# ============================================================

# URLs de tus servidores MCP locales
MCP_LAW_RETRIEVER_URL = "http://localhost:8001/law_lookup"
MCP_CLASSIFIER_URL = "http://localhost:8002/classify_clauses"

# Tiempo m√°ximo de espera por respuesta (segundos)
MCP_TIMEOUT = 10

# ============================================================
# GESTOR MCP TOOLS
# ============================================================

class MCPToolsManager:
    """Gestor de herramientas MCP."""
    
    def __init__(self):
        # Verificaci√≥n b√°sica de conexi√≥n (opcional)
        pass

    def classify_clauses(self, contract_text: str) -> Dict[str, Any]:
        """
        Llama a la herramienta 'classify_clauses' del servidor MCP (8002).
        
        Args:
            contract_text: Texto completo del contrato.
            
        Returns:
            Dict con la clasificaci√≥n de cl√°usulas y riesgos.
        """
        try:
            logger.info("üìû MCP CALL: classify_clauses (Sending contract text...)")
            
            response = requests.post(
                MCP_CLASSIFIER_URL,
                json={"contract_text": contract_text},
                timeout=MCP_TIMEOUT
            )
            
            if response.status_code == 200:
                result = response.json()
                # A veces la respuesta viene anidada, intentamos normalizar
                clauses = result.get('clauses', [])
                logger.info(f"‚úÖ MCP RESPONSE: Classified {len(clauses)} clauses")
                return result
            else:
                logger.error(f"‚ùå MCP ERROR (8002): Status {response.status_code} - {response.text}")
                return {"error": f"Status {response.status_code}", "clauses": []}
                
        except Exception as e:
            logger.error(f"‚ùå MCP CONNECTION ERROR (8002): {e}")
            return {"error": str(e), "clauses": []}
    
    def law_lookup(self, topic: str) -> Dict[str, Any]:
        """
        Llama a la herramienta 'law_lookup' del servidor MCP (8001).
        
        Args:
            topic: T√©rmino de b√∫squeda (ej: "fianza", "despido").
            
        Returns:
            Dict con los art√≠culos legales encontrados.
        """
        try:
            # Limpieza b√°sica del t√©rmino
            topic = topic.strip().lower()
            if not topic:
                return {}

            logger.info(f"üìû MCP CALL: law_lookup ('{topic}')")
            
            response = requests.post(
                MCP_LAW_RETRIEVER_URL,
                json={"topic": topic},
                timeout=MCP_TIMEOUT
            )
            
            if response.status_code == 200:
                result = response.json()
                count = result.get('total_results', 0)
                logger.info(f"‚úÖ MCP RESPONSE: Found {count} laws for '{topic}'")
                return result
            else:
                logger.error(f"‚ùå MCP ERROR (8001): Status {response.status_code} - {response.text}")
                return {"error": f"Status {response.status_code}", "results": []}
                
        except Exception as e:
            logger.error(f"‚ùå MCP CONNECTION ERROR (8001): {e}")
            return {"error": str(e), "results": []}

if __name__ == "__main__":
    # Test r√°pido si se ejecuta directamente
    manager = MCPToolsManager()
    print("üîç Probando conexi√≥n con law_retriever...")
    res = manager.law_lookup("prueba")
    print(f"Resultado: {res}")



==================================================
ARCHIVO: agent\orchestrator.py
==================================================

#!/usr/bin/env python3
"""
agent/orchestrator_with_llm.py
ESTRATEGIA: AGENTE REACT (Reason + Act)
El LLM recibe el texto y DECIDE si llamar herramientas o responder.
"""

import logging
import json
import asyncio
from typing import AsyncIterator, Dict, Any

from agent.llm_client import get_llm_client
from agent.mcp_tools import MCPToolsManager
from agent.pdf_processor import PDFProcessor
from config.nebius_config import NEBIUS_MODEL

logger = logging.getLogger(__name__)

class OrchestratorWithLLM:
    def __init__(self):
        self.llm_client = get_llm_client()
        self.mcp_tools = MCPToolsManager()
        self.pdf_processor = PDFProcessor()

    async def analyze_contract_streaming(self, pdf_path: str) -> AsyncIterator[Dict]:
        """
        Bucle principal del Agente.
        1. Lee PDF
        2. Piensa (LLM)
        3. Si el LLM pide herramienta -> Ejecuta y vuelve a pensar.
        4. Si el LLM da respuesta final -> Termina.
        """
        
        # 1. LEER PDF
        yield {"status": "extracting", "message": "Leyendo documento..."}
        contract_text = self.pdf_processor.extract_text(pdf_path)
        contract_preview = contract_text[:3000] # Limitamos para no saturar contexto r√°pido

        # DEFINICI√ìN DE HERRAMIENTAS PARA EL LLM (SISTEMA PROMPT)
        system_prompt = """Eres Contract Guardian, un auditor experto IA.
        
TIENES DISPONIBLES ESTAS HERRAMIENTAS EXTERNAS (MCP):
1. `consultar_ley(tema)`: Busca leyes oficiales espa√±olas. √ösala ante dudas legales.
2. `clasificar_texto(texto)`: Detecta si un texto es abusivo/riesgoso.

TU PROCESO DE PENSAMIENTO:
1. Analiza el texto del usuario.
2. Si detectas posibles infracciones o dudas, DEBES usar las herramientas.
3. Para usar una herramienta, responde SOLO con este formato JSON:
   {"tool": "consultar_ley", "args": "fianza alquiler maximo"}
   
4. Si ya tienes la informaci√≥n, responde con tu an√°lisis final empezando con "INFORME FINAL:".
"""

        # HISTORIAL DE CONVERSACI√ìN
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Analiza este documento y detecta infracciones:\n\n{contract_preview}..."}
        ]

        # BUCLE DE AGENTE (M√ÅXIMO 3 VUELTAS PARA NO ENTRAR EN LOOP INFINITO)
        for turn in range(3):
            
            # --- PENSAMIENTO DEL LLM ---
            yield {"status": "analyzing", "message": f"El Agente est√° pensando (Paso {turn+1})..."}
            
            # Llamada al LLM (No streaming aqu√≠ para poder procesar JSON)
            response_full = ""
            try:
                # Usamos el cliente raw de openai para tener control total aqu√≠
                stream = self.llm_client.client.chat.completions.create(
                    model=NEBIUS_MODEL,
                    messages=messages,
                    temperature=0.1, # Precisi√≥n para tools
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        response_full += content
                        # Mostrar pensamiento en vivo
                        yield {"status": "analyzing_chunk", "chunk": content}
            
            except Exception as e:
                yield {"status": "error", "message": str(e)}
                return

            # --- DECISI√ìN: ¬øUSAR HERRAMIENTA O TERMINAR? ---
            
            # Intentamos detectar si quiere usar una tool (buscamos JSON)
            tool_call = self._parse_tool_call(response_full)
            
            if tool_call:
                tool_name = tool_call.get("tool")
                tool_args = tool_call.get("args")
                
                yield {"status": "mcp_calls", "message": f"üõ†Ô∏è Ejecutando herramienta: {tool_name}..."}
                
                # EJECUTAR HERRAMIENTA MCP
                tool_result = ""
                if tool_name == "consultar_ley":
                    res = self.mcp_tools.law_lookup(tool_args)
                    tool_result = json.dumps(res, ensure_ascii=False)
                elif tool_name == "clasificar_texto":
                    res = self.mcp_tools.classify_clauses(contract_text[:1000]) # Muestra
                    tool_result = json.dumps(res, ensure_ascii=False)
                
                yield {"status": "mcp_done", "message": "Datos obtenidos."}
                
                # A√ëADIR RESULTADO AL CONTEXTO Y SEGUIR
                messages.append({"role": "assistant", "content": response_full})
                messages.append({"role": "user", "content": f"RESULTADO DE HERRAMIENTA ({tool_name}): {tool_result}. Contin√∫a tu an√°lisis."})
                
            else:
                # Si no pide herramienta, asumimos que es la respuesta final
                yield {
                    "status": "complete", 
                    "result": self._create_dummy_result(contract_text, response_full)
                }
                break

    def _parse_tool_call(self, text):
        """Intenta encontrar un JSON de llamada a herramienta en el texto del LLM."""
        try:
            # Buscamos el primer { y el √∫ltimo }
            start = text.find("{")
            end = text.rfind("}") + 1
            if start != -1 and end != -1:
                json_str = text[start:end]
                return json.loads(json_str)
        except:
            return None
        return None

    def _create_dummy_result(self, text, analysis):
        """
        Crea un resultado estructurado analizando el texto generado por el LLM.
        Intenta separar el informe en secciones l√≥gicas para la UI.
        """
        # 1. Intentar separar Recomendaciones
        recommendations = ""
        reasoning = analysis
        initial = "An√°lisis realizado mediante Agente ReAct con herramientas MCP."
        
        if "Recomendaci√≥n" in analysis or "Conclusi√≥n" in analysis:
            # Buscamos d√≥nde empieza la conclusi√≥n/recomendaci√≥n
            parts = analysis.split("Conclusi√≥n")
            if len(parts) > 1:
                reasoning = parts[0]
                recommendations = "Conclusi√≥n" + parts[1]
            else:
                parts = analysis.split("Recomendaciones")
                if len(parts) > 1:
                    reasoning = parts[0]
                    recommendations = "Recomendaciones" + parts[1]

        # 2. Contar infracciones para los contadores (Heur√≠stica simple)
        # Contamos palabras clave como "Infracci√≥n", "Incorrecto", "Abusiva"
        keywords_high = ["ilegal", "fraude", "infracci√≥n", "abusiva", "grave"]
        keywords_medium = ["incorrecto", "error", "revisar"]
        
        analysis_lower = analysis.lower()
        high_risk = sum(analysis_lower.count(w) for w in keywords_high)
        medium_risk = sum(analysis_lower.count(w) for w in keywords_medium)
        
        # Ajuste para no contar demasiado (un texto puede repetir la misma palabra)
        high_risk = min(high_risk, 5) 
        medium_risk = min(medium_risk, 5)
        total_clauses = high_risk + medium_risk

        from dataclasses import dataclass
        @dataclass
        class Result:
            initial_analysis: str
            mcp_classification: dict
            mcp_laws: dict
            llm_reasoning: str
            recommendations: str
            total_clauses: int
            high_risk_count: int
            medium_risk_count: int
            low_risk_count: int

        return Result(
            initial_analysis=initial,
            mcp_classification={}, # No usado en modo ReAct
            mcp_laws={},           # No usado en modo ReAct
            llm_reasoning=reasoning,     # Aqu√≠ va el cuerpo del an√°lisis
            recommendations=recommendations if recommendations else "Ver detalles en el razonamiento.",
            total_clauses=total_clauses,
            high_risk_count=high_risk,
            medium_risk_count=medium_risk,
            low_risk_count=0
        )


orchestrator = OrchestratorWithLLM()



==================================================
ARCHIVO: agent\pdf_processor.py
==================================================

#!/usr/bin/env python3
"""
agent/pdf_processor.py
Procesador robusto de PDF usando pypdf.
"""
import logging
from pathlib import Path
import pypdf

logger = logging.getLogger(__name__)

class PDFProcessor:
    def __init__(self):
        pass
    
    def extract_text(self, pdf_path: str) -> str:
        path = Path(pdf_path)
        if not path.exists():
            raise FileNotFoundError(f"No se encuentra el PDF: {pdf_path}")
            
        try:
            logger.info(f"üìÑ Extrayendo texto de: {path.name}")
            text = []
            with open(path, 'rb') as f:
                reader = pypdf.PdfReader(f)
                for i, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        text.append(page_text)
                        
            full_text = "\n".join(text)
            if len(full_text) < 50:
                raise ValueError("El PDF parece ser una imagen o estar vac√≠o.")
                
            return full_text
            
        except Exception as e:
            logger.error(f"Error leyendo PDF: {e}")
            raise e

if __name__ == "__main__":
    print("‚úÖ PDF Processor cargado")



==================================================
ARCHIVO: config\nebius_config.py
==================================================

#!/usr/bin/env python3
"""
config/nebius_config.py

Configuraci√≥n de Nebius API para Contract Guardian Agent
NUNCA COMMITEAR ESTE ARCHIVO (agregar a .gitignore)
"""

import os
from dotenv import load_dotenv

# Cargar variables de entorno desde .env
load_dotenv()

# ============================================================
# NEBIUS API CONFIGURATION
# ============================================================

# URL base de Nebius (OpenAI-compatible)
NEBIUS_API_BASE_URL = "https://api.tokenfactory.nebius.com/v1/"

# API Key desde variable de entorno
NEBIUS_API_KEY = os.getenv("NEBIUS_API_KEY")

# Modelo elegido
NEBIUS_MODEL = "Qwen/Qwen3-30B-A3B-Thinking-2507"

# ============================================================
# PAR√ÅMETROS LLM OPTIMIZATION
# ============================================================

# Temperatura: balance entre precisi√≥n y flexibilidad
NEBIUS_TEMPERATURE = 0.6

# Top P: diversidad de tokens
NEBIUS_TOP_P = 0.95

# Max tokens por respuesta
NEBIUS_MAX_TOKENS = 2048

# Timeout en segundos
NEBIUS_TIMEOUT = 30

# Streaming: usuario ve an√°lisis en vivo
NEBIUS_STREAMING = True

# ============================================================
# CONFIGURACI√ìN CONSOLIDADA
# ============================================================

NEBIUS_CONFIG = {
    "model": NEBIUS_MODEL,
    "temperature": NEBIUS_TEMPERATURE,
    "top_p": NEBIUS_TOP_P,
    "max_tokens": NEBIUS_MAX_TOKENS,
    "stream": NEBIUS_STREAMING,
}

# ============================================================
# VALIDACI√ìN
# ============================================================

def validate_config() -> bool:
    """Valida que la configuraci√≥n est√° lista."""
    
    if not NEBIUS_API_KEY:
        raise ValueError(
            "‚ùå NEBIUS_API_KEY no est√° configurada. "
            "Configura la variable de entorno NEBIUS_API_KEY en .env"
        )
    
    if not NEBIUS_API_BASE_URL:
        raise ValueError(
            "‚ùå NEBIUS_API_BASE_URL no est√° configurada."
        )
    
    return True

# ============================================================
# INFORMACI√ìN DE DEBUGGING
# ============================================================

def print_config_info():
    """Imprime config (sin secrets)."""
    print("\n" + "="*60)
    print("üîê NEBIUS CONFIGURATION")
    print("="*60)
    print(f"Model: {NEBIUS_MODEL}")
    print(f"Temperature: {NEBIUS_TEMPERATURE}")
    print(f"Top P: {NEBIUS_TOP_P}")
    print(f"Max Tokens: {NEBIUS_MAX_TOKENS}")
    print(f"Streaming: {NEBIUS_STREAMING}")
    print(f"API Base URL: {NEBIUS_API_BASE_URL}")
    print(f"API Key: {'*' * 20}... (hidden)")
    print("="*60 + "\n")

if __name__ == "__main__":
    validate_config()
    print_config_info()


==================================================
ARCHIVO: ui\app.py
==================================================

#!/usr/bin/env python3
"""
ui/agent_interface_v2.py

Interfaz gr√°fica profesional con Gradio
Soporta:
- Streaming en tiempo real (pensamiento del LLM)
- Upload de PDF
- Visualizaci√≥n de pasos del agente
- Reporte HTML final
"""

import gradio as gr
import asyncio
import json
import logging
import sys
from pathlib import Path

# A√±adir directorio ra√≠z al path para importar m√≥dulos
sys.path.append(str(Path(__file__).parent.parent))

from agent.orchestrator import orchestrator

# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# L√ìGICA DE LA INTERFAZ
# ============================================================

# En ui/agent_interface_v2.py

async def run_analysis(pdf_file):
    """
    Ejecuta el an√°lisis del contrato conectando con el Orchestrator.
    Es un generador as√≠ncrono para streaming de datos a la UI.
    """
    if pdf_file is None:
        yield {
            status_md: "‚ö†Ô∏è **Por favor, sube un archivo PDF para comenzar.**",
            live_log: "",
            html_report: "",
            json_result: None
        }
        return

    # Estado inicial
    current_log = "üöÄ **Iniciando Agente MCP...**\n\n"
    
    # Variables para acumular el texto de cada secci√≥n
    section_initial = ""
    section_reasoning = ""
    section_recommendations = ""
    
    # Texto completo que se mostrar√° en el log
    full_display_text = ""
    
    try:
        # Iterar sobre el stream del orchestrator
        async for event in orchestrator.analyze_contract_streaming(pdf_file.name):
            status = event.get("status")
            
            # --- LOGS DE ESTADO (MENSJES CORTOS) ---
            if status in ["extracting", "analyzing", "extracting_terms", "mcp_calls", 
                          "mcp_done", "reasoning", "recommendations", "generating_report"]:
                
                icon_map = {
                    "extracting": "üìÑ", "analyzing": "ü§ñ", "extracting_terms": "üß†",
                    "mcp_calls": "üåç", "mcp_done": "‚úÖ", "reasoning": "‚öñÔ∏è",
                    "recommendations": "üí°", "generating_report": "üìä"
                }
                icon = icon_map.get(status, "üëâ")
                current_log += f"{icon} {event['message']}...\n"
                
                # Si cambiamos de fase principal, a√±adimos cabecera al log visual
                if status == "analyzing":
                    full_display_text += "\n=== ü§ñ AN√ÅLISIS INICIAL DEL LLM ===\n"
                elif status == "reasoning":
                    full_display_text += "\n\n=== ‚öñÔ∏è RAZONAMIENTO LEGAL (VERIFICACI√ìN) ===\n"
                elif status == "recommendations":
                    full_display_text += "\n\n=== üí° RECOMENDACIONES ===\n"
                
                yield {status_md: current_log, live_log: full_display_text}

            # --- STREAMING DE CONTENIDO (CHUNKS) ---
            
            # 1. Chunk de An√°lisis Inicial
            elif status == "analyzing_chunk":
                chunk = event.get("chunk", "")
                if chunk:
                    section_initial += chunk
                    full_display_text += chunk
                    yield {live_log: full_display_text}

            # 2. Chunk de Razonamiento
            elif status == "reasoning_chunk":
                chunk = event.get("chunk", "")
                if chunk:
                    section_reasoning += chunk
                    full_display_text += chunk
                    yield {live_log: full_display_text}

            # 3. Chunk de Recomendaciones
            elif status == "recommendations_chunk":
                chunk = event.get("chunk", "")
                if chunk:
                    section_recommendations += chunk
                    full_display_text += chunk
                    yield {live_log: full_display_text}

            # --- FINALIZACI√ìN ---
            elif status == "complete":
                current_log += "‚ú® **¬°An√°lisis Completado!**"
                result = event["result"]
                
                # Generar HTML final
                final_html = generate_html(result)
                
                # Generar JSON final
                final_json = {
                    "summary": result.initial_analysis,
                    "classification": result.mcp_classification,
                    "laws": result.mcp_laws,
                    "reasoning": result.llm_reasoning,
                    "recommendations": result.recommendations,
                    "risks": {
                        "high": result.high_risk_count,
                        "medium": result.medium_risk_count,
                        "low": result.low_risk_count
                    }
                }
                
                yield {
                    status_md: current_log,
                    live_log: full_display_text, # Asegurar que el log final est√© completo
                    html_report: final_html,
                    json_result: final_json
                }

            # ERROR HANDLING
            elif status == "error":
                current_log += f"\n‚ùå **ERROR:** {event['message']}"
                yield {status_md: current_log}

    except Exception as e:
        logger.error(f"UI Error: {e}", exc_info=True)
        yield {status_md: f"‚ùå Error cr√≠tico en UI: {str(e)}"}



def generate_html(result):
    """Genera el reporte visual HTML compatible con modo oscuro."""
    return f"""
    <div style="font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; max-width: 800px; margin: 0 auto; color: inherit;">
        
        <!-- HEADER -->
        <div style="background: linear-gradient(135deg, #2563eb 0%, #1e40af 100%); color: white; padding: 30px; border-radius: 12px; margin-bottom: 30px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
            <h1 style="margin: 0; font-size: 24px;">üõ°Ô∏è Contract Guardian Report</h1>
            <p style="margin: 10px 0 0 0; opacity: 0.9;">An√°lisis Legal Potenciado por Agente MCP + LLM</p>
        </div>

        <!-- STATS GRID -->
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 20px; margin-bottom: 30px;">
            <div style="background: var(--background-fill-secondary); padding: 20px; border-radius: 10px; text-align: center; border: 1px solid var(--border-color-primary);">
                <div style="font-size: 32px; font-weight: bold; color: var(--body-text-color);">{result.total_clauses}</div>
                <div style="font-size: 14px; opacity: 0.8; text-transform: uppercase; letter-spacing: 1px; margin-top: 5px;">Riesgos Detectados</div>
            </div>
            <div style="background: rgba(220, 38, 38, 0.1); padding: 20px; border-radius: 10px; text-align: center; border: 1px solid rgba(220, 38, 38, 0.3);">
                <div style="font-size: 32px; font-weight: bold; color: #ef4444;">{result.high_risk_count}</div>
                <div style="font-size: 14px; color: #ef4444; text-transform: uppercase; letter-spacing: 1px; margin-top: 5px;">Riesgo Alto</div>
            </div>
            <div style="background: rgba(217, 119, 6, 0.1); padding: 20px; border-radius: 10px; text-align: center; border: 1px solid rgba(217, 119, 6, 0.3);">
                <div style="font-size: 32px; font-weight: bold; color: #f59e0b;">{result.medium_risk_count}</div>
                <div style="font-size: 14px; color: #f59e0b; text-transform: uppercase; letter-spacing: 1px; margin-top: 5px;">Riesgo Medio</div>
            </div>
            <div style="background: rgba(22, 163, 74, 0.1); padding: 20px; border-radius: 10px; text-align: center; border: 1px solid rgba(22, 163, 74, 0.3);">
                <div style="font-size: 32px; font-weight: bold; color: #22c55e;">{result.low_risk_count}</div>
                <div style="font-size: 14px; color: #22c55e; text-transform: uppercase; letter-spacing: 1px; margin-top: 5px;">Riesgo Bajo</div>
            </div>
        </div>

        <!-- SECTIONS -->
        <!-- Usamos variables CSS de Gradio para que se adapte al tema -->
        <div style="background: var(--background-fill-primary); border-radius: 12px; overflow: hidden; border: 1px solid var(--border-color-primary); margin-bottom: 30px;">
            <div style="background: var(--background-fill-secondary); padding: 15px 25px; border-bottom: 1px solid var(--border-color-primary); font-weight: bold; color: var(--body-text-color); display: flex; align-items: center;">
                ü§ñ AN√ÅLISIS COMPLETO
            </div>
            <div style="padding: 25px; line-height: 1.6; color: var(--body-text-color);">
                {markdown_to_html(result.llm_reasoning)}
            </div>
        </div>

        <div style="background: var(--background-fill-primary); border-radius: 12px; overflow: hidden; border: 1px solid var(--border-color-primary); margin-bottom: 30px;">
            <div style="background: rgba(234, 179, 8, 0.1); padding: 15px 25px; border-bottom: 1px solid rgba(234, 179, 8, 0.3); font-weight: bold; color: #eab308; display: flex; align-items: center;">
                üí° RECOMENDACIONES / CONCLUSI√ìN
            </div>
            <div style="padding: 25px; line-height: 1.6; color: var(--body-text-color);">
                {markdown_to_html(result.recommendations)}
            </div>
        </div>

        <div style="text-align: center; margin-top: 40px; opacity: 0.6; font-size: 13px;">
            Generado por Contract Guardian Agent v2.0 ‚Ä¢ No constituye asesor√≠a legal profesional.
        </div>
    </div>
    """


def markdown_to_html(text):
    """Convierte markdown b√°sico a HTML para visualizaci√≥n simple."""
    if not text: return ""
    html = text.replace("\n", "<br>")
    html = html.replace("**", "<b>").replace("**", "</b>")
    return html


# ============================================================
# INTERFAZ GRADIO
# ============================================================

with gr.Blocks(title="Contract Guardian Agent", theme=gr.themes.Soft(primary_hue="blue", secondary_hue="indigo")) as demo:
    
    # HEADER
    gr.Markdown("""
    # üõ°Ô∏è Contract Guardian - Agente MCP
    ### ü§ñ An√°lisis de Contratos con Inteligencia Artificial + Verificaci√≥n Legal
    """)
    
    with gr.Tabs():
        
        # TAB 1: AN√ÅLISIS PRINCIPAL
        with gr.Tab("üöÄ An√°lisis de Contrato"):
            
            with gr.Row():
                # COLUMNA IZQUIERDA: INPUTS Y ESTADO
                with gr.Column(scale=1):
                    gr.Markdown("### 1. Sube tu contrato")
                    pdf_input = gr.File(
                        label="üìÑ Archivo PDF",
                        file_types=[".pdf"],
                        file_count="single",
                        type="filepath"
                    )
                    
                    analyze_btn = gr.Button(
                        "üöÄ Analizar Ahora", 
                        variant="primary", 
                        size="lg"
                    )
                    
                    gr.Markdown("### üì° Estado del Agente")
                    status_md = gr.Markdown(
                        value="Esperando archivo...",
                        elem_classes="status-box"
                    )
                
                # COLUMNA DERECHA: RESULTADOS EN VIVO
                with gr.Column(scale=2):
                    gr.Markdown("### üí≠ Pensamiento del Agente (En Vivo)")
                    live_log = gr.Textbox(
                        label="Streaming del LLM", 
                        interactive=False, 
                        lines=15,
                        elem_id="live-log",
                        autoscroll=True
                    )

            # SECCI√ìN DE RESULTADOS FINALES
            gr.Markdown("---")
            gr.Markdown("### üìä Resultados del An√°lisis")
            
            with gr.Tabs():
                with gr.Tab("üìë Reporte Visual"):
                    html_report = gr.HTML(label="Reporte Final")
                
                with gr.Tab("üíæ JSON Estructurado"):
                    json_result = gr.JSON(label="Datos Crudos")

        # TAB 2: C√ìMO FUNCIONA
        with gr.Tab("‚ÑπÔ∏è C√≥mo Funciona"):
            gr.Markdown("""
            ## üß† Arquitectura del Agente
            
            Este sistema utiliza una arquitectura **Agentic RAG (Retrieval-Augmented Generation)** potenciada por **MCP (Model Context Protocol)**.
            
            ### El Flujo de Trabajo:
            
            1.  **üìÑ Ingesta**: El agente lee tu PDF y extrae el texto crudo.
            2.  **ü§ñ An√°lisis Inicial (LLM)**: Un modelo de IA (Qwen/DeepSeek) lee el contrato y detecta estructura y cl√°usulas clave.
            3.  **üß† Decisi√≥n**: El agente decide qu√© herramientas necesita para verificar la legalidad.
            4.  **‚ö° Llamadas Paralelas (MCP)**:
                *   `classify_clauses`: Clasifica t√©cnicamente cada cl√°usula.
                *   `law_lookup`: Busca leyes espec√≠ficas (LAU, Estatuto Trabajadores) en tiempo real.
            5.  **‚öñÔ∏è Razonamiento**: El LLM cruza la informaci√≥n del contrato con las leyes encontradas para detectar violaciones.
            6.  **üí° Recomendaci√≥n**: Genera consejos pr√°cticos de negociaci√≥n.
            
            ### Tecnolog√≠as:
            *   **Orquestador**: Python Asyncio
            *   **LLM**: Nebius API (Qwen-32B / DeepSeek-67B)
            *   **Tools**: Protocolo MCP
            *   **Frontend**: Gradio Streaming
            """)

    # EVENTOS
    analyze_btn.click(
        fn=run_analysis,
        inputs=[pdf_input],
        outputs=[status_md, live_log, html_report, json_result]
    )

# Lauch the app
if __name__ == "__main__":
    demo.queue().launch(
        server_name="0.0.0.0", 
        server_port=7860,
        share=False,
        show_error=True
    )



==================================================
ARCHIVO: ui\components.py
==================================================

