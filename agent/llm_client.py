#!/usr/bin/env python3
"""
agent/llm_client.py

Cliente de Nebius LLM para Contract Guardian Agent
Usa OpenAI-compatible client con Qwen3-30B-A3B-Thinking-2507
"""

import logging
from typing import Iterator, Optional
from openai import OpenAI
from config.nebius_config import (
    NEBIUS_API_BASE_URL,
    NEBIUS_API_KEY,
    NEBIUS_CONFIG,
    validate_config,
)

# ============================================================
# CONFIGURACI√ìN LOGGING
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================
# CLIENTE NEBIUS LLM
# ============================================================

class NebiumLLMClient:
    """Cliente para Nebius API con Qwen3."""
    
    def __init__(self):
        """Inicializa cliente de Nebius."""
        try:
            validate_config()
        except ValueError as e:
            logger.error(f"Config validation failed: {e}")
            raise
        
        self.client = OpenAI(
            base_url=NEBIUS_API_BASE_URL,
            api_key=NEBIUS_API_KEY,
        )
        
        logger.info(f"‚úÖ Nebius LLM Client initialized")
        logger.info(f"   Model: {NEBIUS_CONFIG['model']}")
        logger.info(f"   Base URL: {NEBIUS_API_BASE_URL}")
    

        # ... (m√©todos anteriores: analyze_contract, reason_about_clauses, etc.) ...

    def extract_search_terms(self, initial_analysis: str) -> str:
        """
        Pide al LLM que identifique los conceptos legales CLAVE para buscar en la base de datos.
        NO streaming, necesitamos la respuesta completa para procesarla.
        """
        system_prompt = """Eres un asistente legal experto en recuperaci√≥n de informaci√≥n.
Tu tarea es identificar conceptos legales clave para buscar en una base de datos de leyes espa√±olas (Estatuto de los Trabajadores, LAU, etc.).

SALIDA OBLIGATORIA: Solo una lista de 3 a 5 t√©rminos separados por comas. Sin explicaciones, sin puntos finales.
Ejemplo: "despido improcedente, fianza, duraci√≥n del contrato, preaviso"."""

        user_prompt = f"""Basado en este an√°lisis preliminar de un contrato, identifica los 3-5 t√©rminos legales m√°s cr√≠ticos que debemos verificar en la ley para confirmar si hay ilegalidades.

AN√ÅLISIS PRELIMINAR:
{initial_analysis[:2000]}  # Pasamos los primeros 2000 chars para contexto

T√âRMINOS DE B√öSQUEDA:"""

        try:
            logger.info("üîç Asking LLM for search terms...")
            response = self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.3,  # Baja temperatura para ser preciso
                max_tokens=50,
                stream=False      # No streaming, queremos el texto ya
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error extracting search terms: {e}")
            # Fallback por si falla el LLM
            return "terminaci√≥n, responsabilidad, pago"



    def analyze_contract(self, contract_text: str) -> Iterator[str]:
        """
        Analiza un contrato con streaming.
        
        Args:
            contract_text: Texto del contrato a analizar
            
        Yields:
            Chunks de an√°lisis del LLM (streaming)
        """
        
        if not contract_text or len(contract_text.strip()) < 50:
            logger.warning("Contract text too short")
            return
        
        system_prompt = """Eres un abogado experto en derecho espa√±ol con 20 a√±os de experiencia.

Tu tarea: Analizar contratos y identificar cl√°usulas riesgosas o ilegales.

AN√ÅLISIS A REALIZAR:
1. Identificar tipos de cl√°usulas (terminaci√≥n, pago, privacidad, etc.)
2. Detectar nivel de riesgo (ALTO/MEDIO/BAJO)
3. Se√±alar potenciales violaciones legales
4. Sugerir art√≠culos legales aplicables
5. Proporcionar recomendaciones

FORMATO RESPUESTA:
- S√© conciso pero preciso (m√°x 500 palabras)
- Estructura: Tipo | Riesgo | Problema | Art√≠culos | Recomendaci√≥n
- Usa markdown para claridad
- N√∫meros de cl√°usulas si las hay

TONO: Profesional, directo, sin alarmismo pero honesto sobre riesgos"""
        
        user_prompt = f"""Por favor, analiza este contrato e identifica cl√°usulas riesgosas o ilegales seg√∫n la ley espa√±ola.

CONTRATO:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{contract_text}

Proporciona un an√°lisis detallado de los riesgos legales encontrados."""
        
        try:
            logger.info("ü§ñ Sending analysis request to Nebius LLM (streaming)...")
            
            with self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=NEBIUS_CONFIG["temperature"],
                top_p=NEBIUS_CONFIG.get("top_p", 0.95),
                max_tokens=NEBIUS_CONFIG["max_tokens"],
                stream=True,
            ) as stream:
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        
        except Exception as e:
            logger.error(f"Error in analyze_contract: {e}")
            raise
    
    def reason_about_clauses(self, 
                            clauses_summary: str,
                            mcp_results: str) -> Iterator[str]:
        """
        Razona sobre cl√°usulas basado en resultados de MCP tools.
        
        Args:
            clauses_summary: Resumen de cl√°usulas detectadas
            mcp_results: Resultados de law_lookup + classify_clauses
            
        Yields:
            Chunks de razonamiento legal (streaming)
        """
        
        system_prompt = """Eres un abogado especialista en analizar y razonar sobre la legalidad de cl√°usulas contractuales.

Tu tarea: Dado un an√°lisis inicial y verificaci√≥n legal, genera razonamiento profundo sobre violaciones.

ESTRUCTURA RESPUESTA:
- Por cada cl√°usula problem√°tica:
  * Qu√© dice la cl√°usula
  * Qu√© dice la ley
  * Por qu√© es violaci√≥n o ilegal
  * Impacto legal
  * Recomendaci√≥n espec√≠fica"""
        
        user_prompt = f"""Bas√°ndote en estos resultados de an√°lisis legal, razona sobre por qu√© estas cl√°usulas son problem√°ticas:

RESUMEN CL√ÅUSULAS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{clauses_summary}

VERIFICACI√ìN LEGAL (de MCP tools):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{mcp_results}

Genera razonamiento detallado sobre la legalidad de cada cl√°usula."""
        
        try:
            logger.info("üß† Requesting legal reasoning from Nebius LLM (streaming)...")
            
            with self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=NEBIUS_CONFIG["temperature"],
                top_p=NEBIUS_CONFIG.get("top_p", 0.95),
                max_tokens=NEBIUS_CONFIG["max_tokens"],
                stream=True,
            ) as stream:
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        
        except Exception as e:
            logger.error(f"Error in reason_about_clauses: {e}")
            raise
    
    def generate_recommendations(self, analysis_data: str) -> Iterator[str]:
        """
        Genera recomendaciones personalizadas basadas en an√°lisis.
        
        Args:
            analysis_data: Datos de an√°lisis completo
            
        Yields:
            Chunks de recomendaciones (streaming)
        """
        
        system_prompt = """Eres un asesor legal experto en negociaci√≥n de contratos.

Tu tarea: Bas√°ndote en an√°lisis legal, generar recomendaciones pr√°cticas y accionables."""
        
        user_prompt = f"""Bas√°ndote en este an√°lisis, genera recomendaciones pr√°cticas para el cliente:

AN√ÅLISIS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{analysis_data}

Por favor proporciona:
1. Cl√°usulas a RECHAZAR (cr√≠ticas)
2. Cl√°usulas a NEGOCIAR (importantes)
3. Cl√°usulas ACEPTABLES (sin problemas)
4. Estrategia de negociaci√≥n recomendada"""
        
        try:
            logger.info("üí° Requesting recommendations from Nebius LLM (streaming)...")
            
            with self.client.chat.completions.create(
                model=NEBIUS_CONFIG["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=NEBIUS_CONFIG["temperature"],
                top_p=NEBIUS_CONFIG.get("top_p", 0.95),
                max_tokens=NEBIUS_CONFIG["max_tokens"],
                stream=True,
            ) as stream:
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        
        except Exception as e:
            logger.error(f"Error in generate_recommendations: {e}")
            raise


# ============================================================
# INSTANCIA GLOBAL
# ============================================================

llm_client: Optional[NebiumLLMClient] = None

def get_llm_client() -> NebiumLLMClient:
    """Obtiene o crea instancia del cliente LLM."""
    global llm_client
    if llm_client is None:
        llm_client = NebiumLLMClient()
    return llm_client


if __name__ == "__main__":
    # Test b√°sico
    client = get_llm_client()
    print("‚úÖ LLM Client initialized successfully")